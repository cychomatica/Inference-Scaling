{
    "math_psa": 
        {
            "name": "Math-PSA",
            "details": "Qwen2.5-Math-7B-Instruct + LoRA on PRM800K, Math-Shephard and MATH-APS"
        },
    "math_shepherd": 
        {
            "name": "Math-Shepherd",
            "details": "Mistral-7B + Full finetuning on Math-Shephard"
        },
    "qwen2.5_math_7b_prm800k": 
        {
            "name": "Qwen-2.5-Math-PRM",
            "details": "Qwen2.5-Math-7B-Instruct + Full finetuning on PRM800K"
        },
    "rlhflow_deepseek": 
        {
            "name": "RLHFlow-Deepseek",
            "details": "Llama3.1-8B-Instruct + Full finetuning on RLHFlow/Deepseek-PRM-Data"
        },
    "prm800k_llama_fulltune": 
        {
            "name": "LlamaMath-PRM",
            "details": "Llama-3.1-8B-Instruct + Full finetuning on PRM800K"
        },
    "prm800k_qwen_fulltune": 
        {
            "name": "QwenMath-PRM",
            "details": "Qwen2.5-Math-7B-Instruct + Full finetuning on PRM800K"
        },
    "mmlu_noaugs_llama_lora": 
        {
            "name": "LlamaMath-MultiDomain-PRM",
            "details": "Llama-3.1-8B-Instruct + Full finetuning on PRM800K + LoRA on MMLU-Pro-Train"
        },
    "mmlu_noaugs_llamabase_lora": 
        {
            "name": "Llama-MultiDomain-PRM",
            "details": "Llama-3.1-8B-Instruct + LoRA on MMLU-Pro-Train"
        },
    "mmlu_augs_llama_lora": 
        {
            "name": "LlamaMath-AugMultiDomain-PRM",
            "details": "Llama-3.1-8B-Instruct + Full finetuning on PRM800K + LoRA on MMLU-Pro-Train & MMLU-Pro Counterfactual Augmentations"
        },
    "mmlu_onlyaugs_llama_lora": 
        {
            "name": "LlamaMath-OnlyAugMultiDomain-PRM",
            "details": "Llama-3.1-8B-Instruct + Full finetuning on PRM800K + LoRA on MMLU-Pro-Train Counterfactual Augmentations"
        },
    "mmlu_small_noaugs_llama_lora": "Llama-3.1-8B-Instruct \n+ Fulltuning on PRM800K \n+ LoRA on MMLU-Pro-Train Random Subset",
    "mmlu_math_noaugs_llama_lora": "Llama-3.1-8B-Instruct \n+ Fulltuning on PRM800K \n+ LoRA on MMLU-Pro-Train Math Subset",
    "mmlu_noaugs_llama_fulltune": "Llama-3.1-8B-Instruct \n+ Fulltuning on PRM800K \n+ Fulltuning on MMLU-Pro-Train",
    "mmlu_noaugs_qwen_lora": "Qwen2.5-Math-7B-Instruct \n+ Fulltuning on PRM800K \n+ LoRA on MMLU-Pro-Train"
}